{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "import heapq\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "from numpy.core.multiarray import dtype\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.core import Dense, Activation, Dropout, RepeatVector\n",
    "from keras.optimizers import RMSprop\n",
    "from pylab import rcParams\n",
    "\n",
    "matplotlib.use('agg')\n",
    "np.random.seed(42)\n",
    "# tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length:  581888\n"
     ]
    }
   ],
   "source": [
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "rcParams['figure.figsize'] = 12, 5\n",
    "\n",
    "\n",
    "#Loading the data\n",
    "path = '1661-0.txt'\n",
    "text = open(path, \"r\", encoding='utf-8').read().lower()\n",
    "print ('Corpus length: ',len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique chars:  73\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing\n",
    "#Finding all the unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "print (\"unique chars: \",len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num training examples:  193950\n"
     ]
    }
   ],
   "source": [
    "#Cutting the corpus into chunks of 39 chars, spacing the sequences by 3 characters\n",
    "#We will additionally store the next character (the one we need to predict) for every sequence\n",
    "\n",
    "SEQUENCE_LENGTH = 39\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - SEQUENCE_LENGTH, step):\n",
    "    sentences.append(text[i:i+SEQUENCE_LENGTH])\n",
    "    next_chars.append(text[i+SEQUENCE_LENGTH])\n",
    "print ('num training examples: ',len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating features and labels.\n",
    "#Using previously generated sequences and characters that need to be predicted to create one-hot encoded vectors\n",
    "\n",
    "X = np.zeros((len(sentences), SEQUENCE_LENGTH, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the model\n",
    "\n",
    "model = Sequential();\n",
    "model.add(LSTM(128, input_shape=(SEQUENCE_LENGTH, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 184252 samples, validate on 9698 samples\n",
      "Epoch 1/20\n",
      "184252/184252 [==============================] - 117s 633us/step - loss: 1.9578 - accuracy: 0.4243 - val_loss: 2.1103 - val_accuracy: 0.4095\n",
      "Epoch 2/20\n",
      "184252/184252 [==============================] - 107s 581us/step - loss: 1.6110 - accuracy: 0.5174 - val_loss: 2.0765 - val_accuracy: 0.4250\n",
      "Epoch 3/20\n",
      "184252/184252 [==============================] - 115s 626us/step - loss: 1.5138 - accuracy: 0.5454 - val_loss: 2.0468 - val_accuracy: 0.4467\n",
      "Epoch 4/20\n",
      "184252/184252 [==============================] - 110s 598us/step - loss: 1.4618 - accuracy: 0.5602 - val_loss: 2.0261 - val_accuracy: 0.4549\n",
      "Epoch 5/20\n",
      "184252/184252 [==============================] - 105s 571us/step - loss: 1.4244 - accuracy: 0.5683 - val_loss: 2.0470 - val_accuracy: 0.4551\n",
      "Epoch 6/20\n",
      "184252/184252 [==============================] - 82s 444us/step - loss: 1.4006 - accuracy: 0.5749 - val_loss: 2.0182 - val_accuracy: 0.4616\n",
      "Epoch 7/20\n",
      "184252/184252 [==============================] - 64s 345us/step - loss: 1.3824 - accuracy: 0.5797 - val_loss: 2.0514 - val_accuracy: 0.4641\n",
      "Epoch 8/20\n",
      "184252/184252 [==============================] - 71s 384us/step - loss: 1.3654 - accuracy: 0.5844 - val_loss: 2.0375 - val_accuracy: 0.4667\n",
      "Epoch 9/20\n",
      "184252/184252 [==============================] - 60s 323us/step - loss: 1.3539 - accuracy: 0.5866 - val_loss: 2.0463 - val_accuracy: 0.4570\n",
      "Epoch 10/20\n",
      "184252/184252 [==============================] - 61s 331us/step - loss: 1.3424 - accuracy: 0.5894 - val_loss: 2.0309 - val_accuracy: 0.4676\n",
      "Epoch 11/20\n",
      "184252/184252 [==============================] - 60s 325us/step - loss: 1.3338 - accuracy: 0.5919 - val_loss: 2.0565 - val_accuracy: 0.4656\n",
      "Epoch 12/20\n",
      "184252/184252 [==============================] - 62s 336us/step - loss: 1.3241 - accuracy: 0.5949 - val_loss: 2.0468 - val_accuracy: 0.4706\n",
      "Epoch 13/20\n",
      "184252/184252 [==============================] - 69s 376us/step - loss: 1.3184 - accuracy: 0.5967 - val_loss: 2.0562 - val_accuracy: 0.4675\n",
      "Epoch 14/20\n",
      "184252/184252 [==============================] - 67s 362us/step - loss: 1.3108 - accuracy: 0.5982 - val_loss: 2.0982 - val_accuracy: 0.4648\n",
      "Epoch 15/20\n",
      "184252/184252 [==============================] - 93s 502us/step - loss: 1.3072 - accuracy: 0.5983 - val_loss: 2.0753 - val_accuracy: 0.4621\n",
      "Epoch 16/20\n",
      "184252/184252 [==============================] - 105s 572us/step - loss: 1.3000 - accuracy: 0.6012 - val_loss: 2.0823 - val_accuracy: 0.4637\n",
      "Epoch 17/20\n",
      "184252/184252 [==============================] - 105s 570us/step - loss: 1.2949 - accuracy: 0.6020 - val_loss: 2.1237 - val_accuracy: 0.4575\n",
      "Epoch 18/20\n",
      "184252/184252 [==============================] - 108s 585us/step - loss: 1.2903 - accuracy: 0.6024 - val_loss: 2.1014 - val_accuracy: 0.4697\n",
      "Epoch 19/20\n",
      "184252/184252 [==============================] - 110s 595us/step - loss: 1.2859 - accuracy: 0.6044 - val_loss: 2.1097 - val_accuracy: 0.4679\n",
      "Epoch 20/20\n",
      "184252/184252 [==============================] - 111s 600us/step - loss: 1.2830 - accuracy: 0.6054 - val_loss: 2.0975 - val_accuracy: 0.4648\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Training\n",
    "optimizer = RMSprop(lr= 0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X, y, validation_split=0.05, batch_size=128, epochs=20, shuffle=True).history\n",
    "\n",
    "\n",
    "#Saving\n",
    "model.save('keras_model'+str(SEQUENCE_LENGTH)+'.h5')\n",
    "pickle.dump(history, open('history'+str(SEQUENCE_LENGTH)+'.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading back the saved weights and history\n",
    "\n",
    "model = load_model('keras_model'+str(SEQUENCE_LENGTH)+'.h5')\n",
    "history = pickle.load(open('history'+str(SEQUENCE_LENGTH)+'.p', 'rb'))\n",
    "\n",
    "\n",
    "#Evaluation\n",
    "plt.plot(history['accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc= 'upper left')\n",
    "\n",
    "plt.savefig(\"01.Accuracy.png\")\n",
    "\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc= 'upper left')\n",
    "\n",
    "plt.savefig(\"02.Loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "def prepare_input(text):\n",
    "    x = np.zeros((1, SEQUENCE_LENGTH, len(chars)))\n",
    "    for t, char in enumerate(text):\n",
    "        x[0, t, char_indices[char]] = 1\n",
    "    return x\n",
    "#The sequences must be 40 chars long and the tensor is of the shape (1, 40, 57)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The sample function\n",
    "#This function allows us to ask our model what are the next probable characters (The heap simplifies the job)\n",
    "def sample(preds, top_n = 3):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    return heapq.nlargest(top_n, range(len(preds)), preds.take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction function\n",
    "def predict_completion(text):\n",
    "    original_text = text\n",
    "    generalised = text\n",
    "    completion = ''\n",
    "    while True:\n",
    "        x = prepare_input(text)\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, top_n=1)[0]\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        text = text[1:] + next_char\n",
    "        completion += next_char\n",
    "\n",
    "        if len(original_text + completion) + 2 > len(original_text) and next_char == ' ':\n",
    "            return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This methods wraps everything and allows us to predict multiple completions\n",
    "def predict_completions(text, n = 3):\n",
    "    x = prepare_input(text)\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_indices = sample(preds, n)\n",
    "    return [indices_char[idx] + predict_completion(text[1:] + indices_char[idx]) for idx in next_indices]\n",
    "\n",
    "quotes = [\n",
    "    \"It is not a lack of love, but a lack of friendship that makes unhappy marriages.\",\n",
    "    \"That which does not kill us makes us stronger.\",\n",
    "    \"I'm not upset that you lied to me, I'm upset that from now on I can't believe you.\",\n",
    "    \"And those who were seen dancing were thought to be insane by those who could not hear the music.\",\n",
    "    \"It is hard enough to remember my opinions, without also remembering my reasons for them!\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is not a lack of love, but a lack of\n",
      "[' the ', '\\nthe ', 'fer ', ', ', 'mergure ']\n",
      "\n",
      "that which does not kill us makes us st\n",
      "['artled ', 'range ', 'epper ', '. ', 'ill ']\n",
      "\n",
      "i'm not upset that you lied to me, i'm \n",
      "['starked ', 'which ', 'and ', 'commitared ', 'thing ']\n",
      "\n",
      "and those who were seen dancing were th\n",
      "['e ', 'is ', 'ree ', 'at ', 'ought ']\n",
      "\n",
      "it is hard enough to remember my opinio\n",
      "['n, ', 't ', 'us ', 'f ', 'r, ']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n       # return [indices_char[idx] + predict_completion(text[1:] + indices_char[idx]) for idx in next_indices]\\n\\n    quotes = [\\n        \"It is not a lack of love, but a lack of friendship that makes unhappy marriages.\",\\n        \"That which does not kill us makes us stronger.\",\\n        \"I\\'m not upset that you lied to me, I\\'m upset that from now on I can\\'t believe you.\",\\n        \"And those who were seen dancing were thought to be insane by those who could not hear the music.\",\\n        \"It is hard enough to remember my opinions, without also remembering my reasons for them!\"\\n    ]\\n\\n    for q in quotes:\\n        seq = q[:SEQUENCE_LENGTH].lower()\\n        print (seq)\\n        print (predict_completions(seq, 5))\\n        '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for q in quotes:\n",
    "    seq = q[:SEQUENCE_LENGTH].lower()\n",
    "    print (seq)\n",
    "    print (predict_completions(seq, 5))\n",
    "    print ()\n",
    "'''\n",
    "       # return [indices_char[idx] + predict_completion(text[1:] + indices_char[idx]) for idx in next_indices]\n",
    "\n",
    "    quotes = [\n",
    "        \"It is not a lack of love, but a lack of friendship that makes unhappy marriages.\",\n",
    "        \"That which does not kill us makes us stronger.\",\n",
    "        \"I'm not upset that you lied to me, I'm upset that from now on I can't believe you.\",\n",
    "        \"And those who were seen dancing were thought to be insane by those who could not hear the music.\",\n",
    "        \"It is hard enough to remember my opinions, without also remembering my reasons for them!\"\n",
    "    ]\n",
    "\n",
    "    for q in quotes:\n",
    "        seq = q[:SEQUENCE_LENGTH].lower()\n",
    "        print (seq)\n",
    "        print (predict_completions(seq, 5))\n",
    "        '''\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/51928456/error-while-changing-the-sequence-length\n",
    "# https://www.analyticsvidhya.com/blog/2021/08/predict-the-next-word-of-your-text-using-long-short-term-memory-lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00e0097842e949ee6d876401c9c78e5790d6572053af66440c4271678bee99a8"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 ('DL_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
